{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference on `Cyclone YAKU` (Visual use-case)\n",
    "* The aim of this notebook is to use the final set of trained models to forecast the next 4-hour sequence for a chosen use case.\n",
    "\n",
    "* The set of models currently implemented is: `DYffusion` (LCB and L1), `ConvLSTM` (LCB, L1) and `STEPS`.\n",
    "\n",
    "* THis NB generates the plot for the paper: https://arxiv.org/abs/2412.02723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import imageio\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xarray as xr\n",
    "import xskillscore as xs\n",
    "from livelossplot import PlotLosses\n",
    "from matplotlib.colors import ListedColormap\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from scipy import io\n",
    "from torch.nn import L1Loss, MSELoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.image import LearnedPerceptualImagePatchSimilarity as LPIPS\n",
    "from torchmetrics.regression import CriticalSuccessIndex\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rainnow.src.conv_lstm_utils import (\n",
    "    IMERGDataset,\n",
    "    generate_sequence_conv_lstm,\n",
    "    plot_predicted_sequence,\n",
    "    save_checkpoint,\n",
    "    train,\n",
    "    validate,\n",
    ")\n",
    "from rainnow.src.dyffusion.experiment_types.forecasting_multi_horizon import (\n",
    "    MultiHorizonForecastingDYffusion,\n",
    ")\n",
    "from rainnow.src.dyffusion.experiment_types.interpolation import InterpolationExperiment\n",
    "from rainnow.src.dyffusion.utilities.evaluation import (\n",
    "    evaluate_ensemble_crps,\n",
    "    evaluate_ensemble_spread_skill_ratio,\n",
    ")\n",
    "from rainnow.src.loss import CBLoss, LPIPSMSELoss\n",
    "from rainnow.src.models.conv_lstm import ConvLSTMModel\n",
    "from rainnow.src.normalise import PreProcess\n",
    "from rainnow.src.plotting import (\n",
    "    plot_a_sequence,\n",
    "    plot_an_interpolated_sequence,\n",
    ")\n",
    "from rainnow.src.utilities.instantiators import instantiate_multi_horizon_dyffusion_model\n",
    "from rainnow.src.utilities.loading import (\n",
    "    get_model_ckpt_path,\n",
    "    instantiate_interpolator_model_ckpt_from_config,\n",
    "    load_imerg_datamodule_from_config,\n",
    "    load_model_checkpoint_for_eval,\n",
    "    load_model_state_dict,\n",
    ")\n",
    "from rainnow.src.utilities.utils import (\n",
    "    get_device,\n",
    "    transform_0_1_to_minus1_1,\n",
    "    transform_minus1_1_to_0_1,\n",
    ")\n",
    "\n",
    "from rainnow.src.pysteps_steps import PyStepsNowcastModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence_dyffusion(\n",
    "    model: MultiHorizonForecastingDYffusion, inputs: torch.Tensor\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate a sequence using a DYffusion model.\n",
    "\n",
    "    This function uses the provided DYffusion model to sample a sequence\n",
    "    based on the given initial condition.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : MultiHorizonForecastingDYffusion\n",
    "        The DYffusion model to use for prediction.\n",
    "    inputs : torch.Tensor\n",
    "        The input tensor to use as the initial condition for sampling.\n",
    "    \"\"\"\n",
    "    predictions = model.model.sample(initial_condition=inputs)\n",
    "    if isinstance(model.model_final_layer_activation, nn.Tanh):\n",
    "        predictions = {k: transform_minus1_1_to_0_1(v) for k, v in predictions.items()}\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `helpers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** DIR helpers **\n",
    "BASE_PATH = \"/teamspace/studios/this_studio\"\n",
    "\n",
    "CKPT_BASE_PATH = f\"{BASE_PATH}/DYffcast/rainnow/results/\"\n",
    "CONFIGS_BASE_PATH = f\"{BASE_PATH}/DYffcast/rainnow/src/dyffusion/configs/\"\n",
    "\n",
    "CKPT_DIR = \"checkpoints\"\n",
    "CKPT_CFG_NAME = \"hparams.yaml\"\n",
    "DATAMODULE_CONFIG_NAME = \"imerg_precipitation.yaml\"\n",
    "# whether or not to get last.ckpt or to get the \"best model\" ckpt (the other one in the folder).\n",
    "GET_LAST = False\n",
    "\n",
    "# ** Dataloader Params **\n",
    "BATCH_SIZE = 20  # more than we need just to get entire predict sequence in one batch.\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# ** plotting helpers **\n",
    "# cmap = io.loadmat(\"../../src/utilities/cmaps/colormap.mat\")\n",
    "cmap = io.loadmat(f\"{BASE_PATH}/DYffcast/rainnow/src/utilities/cmaps/colormap.mat\")\n",
    "rain_cmap = ListedColormap(cmap[\"Cmap_rain\"])\n",
    "global_params = {\"font.size\": 8}  # , \"font.family\": \"Times New Roman\"}\n",
    "plt_params = {\"wspace\": 0.1, \"hspace\": 0.15}\n",
    "ylabel_params = {\"ha\": \"right\", \"va\": \"bottom\", \"labelpad\": 1, \"fontsize\": 7.5}\n",
    "plot_params = {\"cmap\": rain_cmap, \"vmin\": 0.5, \"vmax\": 10}\n",
    "\n",
    "# ** get device **\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Datamodules & Dataloaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = load_imerg_datamodule_from_config(\n",
    "    cfg_base_path=CONFIGS_BASE_PATH,\n",
    "    cfg_name=DATAMODULE_CONFIG_NAME,\n",
    "    overrides={\n",
    "        \"boxes\": [\"1,0\"],  # [\"0,0\", \"1,0\", \"2,0\", \"2,1\"],\n",
    "        \"window\": 1,\n",
    "        \"horizon\": 8,\n",
    "        \"prediction_horizon\": 8,\n",
    "        \"sequence_dt\": 1,\n",
    "    },\n",
    ")\n",
    "datamodule.setup(\"predict\")\n",
    "\n",
    "# set up the dataloaders.\n",
    "predict_dataloader = DataLoader(datamodule._data_predict, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of data.\n",
    "iter_loader = iter(predict_dataloader)\n",
    "# iter_loader = iter(val_dataloader)\n",
    "X = next(iter_loader)[\"dynamics\"]\n",
    "\n",
    "# debug info.\n",
    "print(f\"\\n** batch size = {BATCH_SIZE} **\")\n",
    "print(f\"** num batches = {len(iter_loader)} **\")\n",
    "print(f\"** num samples = {BATCH_SIZE * len(iter_loader)} ** \")\n",
    "print(f\"** sample dims: {X.shape}\")  # (batch_size, sequence_length, channels, H, W)\n",
    "print(f\"** loaded sequence length = {datamodule.hparams.horizon + datamodule.hparams.window} **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Instantiate the preprocessor object`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** instantiate the preprocesser obj **\n",
    "pprocessor = PreProcess(\n",
    "    percentiles=datamodule.normalization_hparams[\"percentiles\"],\n",
    "    minmax=datamodule.normalization_hparams[\"min_max\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all the models.\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Load in DYffusion models.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ckpt_id, model_key_name in [\n",
    "    (\"dyffusion-daftvdwg\", \"DYffusion_LCB\"),\n",
    "    (\"dyffusion-fyxpjp65\", \"DYffusion_L1\"),\n",
    "]:\n",
    "    # ** instantiate a DYffusion model **\n",
    "    model = instantiate_multi_horizon_dyffusion_model(\n",
    "        ckpt_id=ckpt_id,\n",
    "        ckpt_base_path=CKPT_BASE_PATH,\n",
    "        diffusion_config_overrides={\n",
    "            \"interpolator_checkpoint_base_path\": f\"{BASE_PATH}/DYffcast/rainnow/results/\"\n",
    "        },\n",
    "    )\n",
    "    # load in model checkpoint.\n",
    "    ckpt_path = get_model_ckpt_path(ckpt_id=ckpt_id, ckpt_base_path=CKPT_BASE_PATH, get_last=False)\n",
    "    print(f\"** loading ckpt id from: {ckpt_path} **\")\n",
    "    state_dict = load_model_state_dict(ckpt_path=ckpt_path, device=device)\n",
    "    model._model.load_state_dict(state_dict)\n",
    "    # set model into eval mode.\n",
    "    model.eval()\n",
    "    model._model.eval()\n",
    "\n",
    "    # add DYffusion model to the global model store.\n",
    "    models[model_key_name] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Load in the ConvLSTM Models` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvLSTM params (make sure that they match up with the model checkpoint).\n",
    "INPUT_SEQUENCE_LENGTH = 4\n",
    "OUTPUT_SEQUENCE_LENGTH = 1\n",
    "IMG_DIMS = (1, 128, 128)\n",
    "\n",
    "ckpt_params = {\n",
    "    \"convlstm-a8kwo8jx\": {\n",
    "        \"input_sequence_length\": INPUT_SEQUENCE_LENGTH,\n",
    "        \"output_sequence_length\": OUTPUT_SEQUENCE_LENGTH,\n",
    "        \"kernel_size\": (5, 5),\n",
    "        \"input_dims\": IMG_DIMS,  # C, H, W\n",
    "        \"output_channels\": 1,\n",
    "        \"hidden_channels\": [128, 128],\n",
    "        \"num_layers\": 2,\n",
    "        \"output_activation\": nn.Sigmoid(),\n",
    "    },\n",
    "    \"convlstm-k34y14il\": {\n",
    "        \"input_sequence_length\": INPUT_SEQUENCE_LENGTH,\n",
    "        \"output_sequence_length\": OUTPUT_SEQUENCE_LENGTH,\n",
    "        \"kernel_size\": (5, 5),\n",
    "        \"input_dims\": IMG_DIMS,  # C, H, W\n",
    "        \"output_channels\": 1,\n",
    "        \"hidden_channels\": [128, 128],\n",
    "        \"num_layers\": 2,\n",
    "        \"output_activation\": nn.Sigmoid(),\n",
    "    },\n",
    "    \"convlstm-abcd1234\": {\n",
    "        \"input_sequence_length\": INPUT_SEQUENCE_LENGTH,\n",
    "        \"output_sequence_length\": OUTPUT_SEQUENCE_LENGTH,\n",
    "        \"kernel_size\": (5, 5),\n",
    "        \"input_dims\": IMG_DIMS,  # C, H, W\n",
    "        \"output_channels\": 1,\n",
    "        \"hidden_channels\": [128, 128],\n",
    "        \"num_layers\": 2,\n",
    "        \"output_activation\": nn.Tanh(),\n",
    "    },\n",
    "    \"convlstm-i93g6pzb\": {\n",
    "        \"input_sequence_length\": INPUT_SEQUENCE_LENGTH,\n",
    "        \"output_sequence_length\": OUTPUT_SEQUENCE_LENGTH,\n",
    "        \"kernel_size\": (5, 5),\n",
    "        \"input_dims\": IMG_DIMS,  # C, H, W\n",
    "        \"output_channels\": 1,\n",
    "        \"hidden_channels\": [128, 128],\n",
    "        \"num_layers\": 2,\n",
    "        \"output_activation\": nn.Tanh(),\n",
    "    },\n",
    "}\n",
    "\n",
    "for ckpt_id, model_key_name in [\n",
    "    (\"convlstm-abcd1234\", \"ConvLSTM_LCB\"),\n",
    "    # (\"convlstm-i93g6pzb\", \"ConvLSTM_LCB\"),\n",
    "    # (\"convlstm-a8kwo8jx\", \"ConvLSTM_BCE\"),\n",
    "    (\"convlstm-k34y14il\", \"ConvLSTM_BCE\"),\n",
    "]:\n",
    "    # get model ckpt path.\n",
    "    ckpt_id_path = f\"{BASE_PATH}/DYffcast/rainnow/results/{ckpt_id}/checkpoints/{ckpt_id}.pt\"\n",
    "    print(f\"** loading ckpt id from: {ckpt_id_path} **\")\n",
    "\n",
    "    # instantiate a new ConvLSTM model.\n",
    "    model = ConvLSTMModel(\n",
    "        input_sequence_length=ckpt_params[ckpt_id][\"input_sequence_length\"],\n",
    "        output_sequence_length=ckpt_params[ckpt_id][\"output_sequence_length\"],\n",
    "        input_dims=ckpt_params[ckpt_id][\"input_dims\"],\n",
    "        hidden_channels=ckpt_params[ckpt_id][\"hidden_channels\"],\n",
    "        output_channels=ckpt_params[ckpt_id][\"output_channels\"],\n",
    "        num_layers=ckpt_params[ckpt_id][\"num_layers\"],\n",
    "        kernel_size=ckpt_params[ckpt_id][\"kernel_size\"],\n",
    "        output_activation=ckpt_params[ckpt_id][\"output_activation\"],\n",
    "        device=device,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    # load in the checkpoint + set to eval() mode.\n",
    "    model.load_state_dict(\n",
    "        state_dict=torch.load(ckpt_id_path, map_location=torch.device(device))[\"model_state_dict\"]\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    # add ConvLSTM model to the global model store.\n",
    "    models[model_key_name] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add in the `STEPS` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in the STEPS kwargs (for roll-out inference).\n",
    "models[\"STEPS\"] = {\n",
    "    \"horizon\": 30 * 1,\n",
    "    \"data_frequency\": 30,  # in min\n",
    "    \"spatial_resolution\": 10,  # in km\n",
    "    \"num_ensemble\": 10,\n",
    "    \"convert_to_dbr\": True,\n",
    "    \"input_dims\": (1, 128, 128),\n",
    "    \"nowcast_kwargs\": {\n",
    "        \"mm_h_precip_thd\": 0.1,\n",
    "        \"dbr_precip_thd\": -15,\n",
    "        \"n_cascade_levels\": 6,\n",
    "        \"noise_method\": \"nonparametric\",  # adds stochastic noise to represent growth/decay.\n",
    "        \"vel_pert_method\": \"bps\",  # If set to None, the advection field is not perturbed.\n",
    "        \"mask_method\": \"incremental\",  # \"sprog\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Create the prediction inputs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = X.clone()\n",
    "batch_num = 11\n",
    "# get x0, interpolated values and xh from X.\n",
    "x0 = inputs[batch_num, 0, :, :, :]\n",
    "# get an unprocessed set of inputs for the PySTEPS.\n",
    "x0_STEPS = pprocessor.reverse_processing(x0)\n",
    "\n",
    "targets = pprocessor.reverse_processing(inputs)[batch_num, 1:, ...]\n",
    "\n",
    "# pre-processed sequence.\n",
    "plot_a_sequence(X, b=batch_num, global_params=global_params, plot_params={\"cmap\": rain_cmap})\n",
    "# raw sequence (all preprocessing reversed).\n",
    "plot_a_sequence(\n",
    "    pprocessor.reverse_processing(inputs),\n",
    "    b=batch_num,\n",
    "    global_params=global_params,\n",
    "    plot_params=plot_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ConvLSTM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the ConvLSTM we need to take the last N previous timesteps from batch-1 and attached it to x0.\n",
    "# so that N+x0 = input_sequence_length.\n",
    "prev_batch = batch_num - 1\n",
    "prev_sequence = inputs[batch_num - 1, :, :, :, :]\n",
    "lookback = INPUT_SEQUENCE_LENGTH - 1\n",
    "conv_lstm_inputs = torch.cat([prev_sequence[-lookback:, ...], x0.unsqueeze(0)])\n",
    "print(f\"** ConvLSTM input dims: {conv_lstm_inputs.size()} **\")\n",
    "assert conv_lstm_inputs.size(0) == INPUT_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `STEPS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the STEPS nowcast, we copy the same approach as ConvLSTM but reverse the preprocessing.\n",
    "steps_inputs = torch.cat(\n",
    "    [pprocessor.reverse_processing(prev_sequence[-lookback:, ...]), x0_STEPS.unsqueeze(0)]\n",
    ")\n",
    "print(f\"** STEPS input dims: {steps_inputs.size()} **\")\n",
    "assert steps_inputs.size(0) == INPUT_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `DYffusion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = {\n",
    "    \"DYffusion\": x0.unsqueeze(0),  # (B, C, H, W).\n",
    "    \"ConvLSTM\": conv_lstm_inputs.unsqueeze(0),  # (B, S, C, H, W).\n",
    "    \"STEPS\": steps_inputs,  # (S, N, C, W)\n",
    "}\n",
    "\n",
    "# checks.\n",
    "assert model_inputs[\"DYffusion\"].size() == torch.empty(1, 1, 128, 128).size()\n",
    "assert model_inputs[\"ConvLSTM\"].size() == torch.empty(1, INPUT_SEQUENCE_LENGTH, 1, 128, 128).size()\n",
    "assert model_inputs[\"STEPS\"].size() == torch.empty(INPUT_SEQUENCE_LENGTH, 1, 128, 128).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ENSEMBLE = 10\n",
    "model_predictions = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"** generating sequence using {model_name} **\")\n",
    "    if \"DYffusion\" in model_name:\n",
    "        # generating ensemble predictions (using DYffusion).\n",
    "        ens_preds = {}\n",
    "        for n in range(NUM_ENSEMBLE):\n",
    "            pred = generate_sequence_dyffusion(\n",
    "                model=models[model_name], inputs=model_inputs[\"DYffusion\"]\n",
    "            )\n",
    "            ens_preds[f\"ens_{n+1}\"] = pred\n",
    "        # need to take the mean of the ensemble.\n",
    "        mean_prediction = torch.mean(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    e.unsqueeze(0)\n",
    "                    for _, e in {\n",
    "                        k: torch.cat([q for _, q in v.items()], dim=0) for k, v in ens_preds.items()\n",
    "                    }.items()\n",
    "                ],\n",
    "                dim=0,\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "        model_predictions[model_name] = mean_prediction\n",
    "\n",
    "    elif \"ConvLSTM\" in model_name:\n",
    "        # generate sequence using ConvLSTM.\n",
    "        pred = generate_sequence_conv_lstm(\n",
    "            model=model,\n",
    "            inputs=model_inputs[\"ConvLSTM\"],\n",
    "            horizon=8,\n",
    "            device=device,\n",
    "        )\n",
    "        prediction = torch.cat([v for _, v in pred.items()], dim=0)\n",
    "        model_predictions[model_name] = prediction\n",
    "\n",
    "    elif \"STEPS\" in model_name:\n",
    "        # generate sequence using STEPS.\n",
    "        # pred = generate_sequence_steps()\n",
    "\n",
    "        X_steps = model_inputs[\"STEPS\"]\n",
    "        predictions = {}\n",
    "        for t in range(targets.size(0)):\n",
    "            # instantiate PySteps model using X.\n",
    "            pysteps_model = PyStepsNowcastModel(\n",
    "                input_precip_sequence=X_steps,\n",
    "                input_dims=models[\"STEPS\"][\"input_dims\"],  # c, h, w.\n",
    "                horizon=models[\"STEPS\"][\"horizon\"],  # in min\n",
    "                data_time_interval=models[\"STEPS\"][\"data_frequency\"],\n",
    "                data_km_per_pixel_resolution=models[\"STEPS\"][\"spatial_resolution\"],\n",
    "                num_ensemble=NUM_ENSEMBLE,  # models[\"STEPS\"][\"num_ensemble\"],\n",
    "                transform_mm_h_to_dBR=models[\"STEPS\"][\"convert_to_dbr\"],\n",
    "            )\n",
    "            # STEPS nowcast.\n",
    "            R_f = pysteps_model.nowcast(\n",
    "                mm_h_precip_threshold=models[\"STEPS\"][\"nowcast_kwargs\"][\"mm_h_precip_thd\"],\n",
    "                dBR_precip_threshold=models[\"STEPS\"][\"nowcast_kwargs\"][\"dbr_precip_thd\"],\n",
    "                n_cascade_levels=models[\"STEPS\"][\"nowcast_kwargs\"][\"n_cascade_levels\"],\n",
    "                noise_method=models[\"STEPS\"][\"nowcast_kwargs\"][\"noise_method\"],\n",
    "                vel_pert_method=models[\"STEPS\"][\"nowcast_kwargs\"][\"vel_pert_method\"],\n",
    "                mask_method=models[\"STEPS\"][\"nowcast_kwargs\"][\"mask_method\"],\n",
    "            )\n",
    "\n",
    "            # get mean of ensemble.\n",
    "            pred = torch.from_numpy(np.mean(R_f, axis=0)).unsqueeze(0)\n",
    "\n",
    "            # store the prediction.\n",
    "            predictions[f\"t{t+1}\"] = pred\n",
    "\n",
    "            # update the inputs with the last pred.\n",
    "            X_steps = torch.concat([model_inputs[\"STEPS\"][1:, ...], pred], dim=0)\n",
    "\n",
    "        predictions = torch.cat([v for _, v in predictions.items()], dim=0)\n",
    "        model_predictions[model_name] = predictions\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Only 'DYffusion', 'ConvLSTM' and 'STEPS' are currently handled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `plot the results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(models) + 1\n",
    "ncols = int(targets.size(0) / 2) + 1\n",
    "figsize = (12, 12)\n",
    "fs = 14\n",
    "dt = 2\n",
    "c = 0\n",
    "\n",
    "model_name_formatted = {\n",
    "    \"DYffusion_LCB\": r\"DYffusion$_{\\text{LCB}}$\",\n",
    "    \"DYffusion_L1\": r\"DYffusion$_{\\text{L1}}$\",\n",
    "    \"ConvLSTM_LCB\": r\"ConvLSTM$_{\\text{LCB}}$\",\n",
    "    \"ConvLSTM_BCE\": r\"ConvLSTM$_{\\text{BCE}}$\",\n",
    "    \"STEPS\": r\"STEPS\",\n",
    "}\n",
    "\n",
    "ylabel_params = {\"ha\": \"right\", \"va\": \"bottom\", \"labelpad\": 1, \"fontsize\": 12}\n",
    "\n",
    "# reverse x0 back to raw scale.\n",
    "x0_raw = pprocessor.reverse_processing(x0)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "plt.rcParams.update(global_params)\n",
    "\n",
    "# plot the targets first.\n",
    "for i in range(ncols):\n",
    "    if i == 0:\n",
    "        # plot x0.\n",
    "        axs[0, i].imshow(x0_raw[c, ...], **plot_params)\n",
    "        axs[0, i].set_title(r\"$x_0$\", fontsize=fs)\n",
    "    else:\n",
    "        axs[0, i].imshow(targets[(i * dt) - 1, c, ...], **plot_params)\n",
    "        num_minutes = i * 60\n",
    "        axs[0, i].set_title(f\"$x_{{t+{num_minutes}min}}$\", fontsize=fs)\n",
    "\n",
    "# plot model predictions.\n",
    "for j, (model_name, predictions) in enumerate(model_predictions.items()):\n",
    "    axs[j + 1, 1].set_ylabel(model_name_formatted[model_name], rotation=90, **ylabel_params)\n",
    "    axs[j + 1, 1].yaxis.set_label_coords(-0.05, 1)\n",
    "\n",
    "    if model_name in [\"STEPS\"]:\n",
    "        predictions_reversed = predictions\n",
    "    else:\n",
    "        # get the raw predictions.\n",
    "        predictions_reversed = pprocessor.reverse_processing(predictions)\n",
    "\n",
    "    for i in range(1, ncols):\n",
    "        idx = i * dt - 1\n",
    "        axs[j + 1, i].imshow(predictions_reversed[idx, c, ...].detach().cpu(), **plot_params)\n",
    "\n",
    "# remove the box around each subplot.\n",
    "for j in range(nrows):\n",
    "    for i in range(ncols):\n",
    "        axs[j, i].set_xticks([])\n",
    "        axs[j, i].set_yticks([])\n",
    "for j in range(1, nrows):\n",
    "    axs[j, 0].spines[\"top\"].set_visible(False)\n",
    "    axs[j, 0].spines[\"right\"].set_visible(False)\n",
    "    axs[j, 0].spines[\"bottom\"].set_visible(False)\n",
    "    axs[j, 0].spines[\"left\"].set_visible(False)\n",
    "\n",
    "# add labels to the left hand axes.\n",
    "axs[0, 0].set_ylabel(\"Ground Truth\".upper(), rotation=90, **ylabel_params)\n",
    "axs[0, 0].yaxis.set_label_coords(-0.05, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(**{\"wspace\": 0.1, \"hspace\": 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo params.\n",
    "geo_box = [-83.55, -70.75, -8.75, 4.05]  # this is for [1, 0].\n",
    "lons = np.linspace(-83.55, -70.75, 128)\n",
    "lats = np.linspace(-8.75, 4.05, 128)\n",
    "lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "\n",
    "nrows = len(models) + 1\n",
    "ncols = int(targets.size(0) / 2) + 1\n",
    "figsize = (12, 12)\n",
    "fs = 14\n",
    "dt = 2\n",
    "c = 0\n",
    "\n",
    "# labels = [\"Ground Truth\", \"DYffusion_LCB\", \"DYffusion_L1\", \"ConvLSTM_LCB\", \"ConvLSTM_BCE\"]\n",
    "labels = [\n",
    "    \"Ground Truth\",\n",
    "    r\"DYffusion$_{\\text{LCB}}$\",\n",
    "    r\"DYffusion$_{\\text{L1}}$\",\n",
    "    r\"ConvLSTM$_{\\text{LCB}}$\",\n",
    "    r\"ConvLSTM$_{\\text{BCE}}$\",\n",
    "    r\"STEPS\",\n",
    "]\n",
    "ylabel_params = {\"ha\": \"right\", \"va\": \"bottom\", \"labelpad\": 1, \"fontsize\": 12}\n",
    "\n",
    "# reverse x0 back to raw scale.\n",
    "x0_raw = pprocessor.reverse_processing(x0)\n",
    "\n",
    "fig = plt.figure(figsize=figsize)\n",
    "plt.rcParams.update(global_params)\n",
    "\n",
    "# keep track of all axes.\n",
    "gs = gridspec.GridSpec(nrows, ncols + 1, figure=fig, width_ratios=[0.1] + [1] * ncols)\n",
    "axes = []\n",
    "\n",
    "\n",
    "def add_geo_ax_to_fig(fig, geo_box, nrows, ncols, index, axes):\n",
    "    ax = fig.add_subplot(nrows, ncols, index, projection=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.COASTLINE, edgecolor=\"black\", linewidth=1, alpha=0.75)\n",
    "    ax.add_feature(cfeature.BORDERS, edgecolor=\"darkgrey\", linewidth=1, alpha=0.75)\n",
    "    ax.set_extent(geo_box, crs=ccrs.PlateCarree())\n",
    "    axes.append(ax)\n",
    "    return ax\n",
    "\n",
    "\n",
    "# plot the targets.\n",
    "for i in range(ncols):\n",
    "    if i == 0:\n",
    "        # plot x0.\n",
    "        ax = add_geo_ax_to_fig(\n",
    "            fig=fig, geo_box=geo_box, nrows=nrows, ncols=ncols, index=i + 1, axes=axes\n",
    "        )\n",
    "        im = ax.pcolormesh(\n",
    "            lon_grid, lat_grid, np.flipud(x0_raw[c, ...]), transform=ccrs.PlateCarree(), **plot_params\n",
    "        )\n",
    "        ax.set_title(r\"$x_0$\", fontsize=fs)\n",
    "    else:\n",
    "        ax = add_geo_ax_to_fig(\n",
    "            fig=fig, geo_box=geo_box, nrows=nrows, ncols=ncols, index=i + 1, axes=axes\n",
    "        )\n",
    "        im = ax.pcolormesh(\n",
    "            lon_grid,\n",
    "            lat_grid,\n",
    "            np.flipud(targets[(i * dt) - 1, c, ...]),\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            **plot_params,\n",
    "        )\n",
    "        ax.set_title(f\"$x_{{t+{i*60}min}}$\", fontsize=fs)\n",
    "\n",
    "# plot model predictions.\n",
    "for j, (model_name, predictions) in enumerate(model_predictions.items()):\n",
    "    if model_name in [\"STEPS\"]:\n",
    "        predictions_reversed = predictions\n",
    "    else:\n",
    "        predictions_reversed = pprocessor.reverse_processing(predictions)\n",
    "    for i in range(1, ncols):\n",
    "        ax = add_geo_ax_to_fig(\n",
    "            fig=fig,\n",
    "            geo_box=geo_box,\n",
    "            nrows=nrows,\n",
    "            ncols=ncols,\n",
    "            index=((j + 1) * ncols) + i + 1,\n",
    "            axes=axes,\n",
    "        )\n",
    "        im = ax.pcolormesh(\n",
    "            lon_grid,\n",
    "            lat_grid,\n",
    "            np.flipud(predictions_reversed[(i * dt - 1), c, ...].detach().cpu()),\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            **plot_params,\n",
    "        )\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    label_ax = fig.add_subplot(gs[i, 0])\n",
    "    label_ax.axis(\"off\")\n",
    "    xx, yy = 0.5, 0.5\n",
    "    if i > 0:\n",
    "        xx = 11.5\n",
    "    label_ax.text(xx, yy, label, rotation=90, va=\"center\", ha=\"right\", fontsize=12)\n",
    "\n",
    "# Add single colorbar to the right of all subplots\n",
    "cbar_ax = fig.add_axes([0.91, 0.2, 0.005, 0.6])\n",
    "cbar = plt.colorbar(im, cax=cbar_ax, **{\"extend\": \"max\", \"pad\": 0.05})\n",
    "cbar.set_ticks(np.arange(1, 9, 1))\n",
    "cbar.set_ticklabels([\"0.1\", \"0.5\", \"1\", \"2\", \"4\", \"8\", \"16\", \"32\"], fontsize=10)\n",
    "cbar.ax.text(\n",
    "    3.5,\n",
    "    1.05,\n",
    "    r\"$\\text{mm}\\cdot\\text{h}^{-1}$\",\n",
    "    transform=cbar.ax.transAxes,\n",
    "    ha=\"center\",\n",
    "    va=\"bottom\",\n",
    "    fontsize=10,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(**{\"wspace\": 0.1, \"hspace\": 0.1, \"right\": 0.9})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Prediction GIFs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames per second.\n",
    "fs = 20\n",
    "\n",
    "# plot model predictions.\n",
    "gifs = {}\n",
    "for j, (model_name, predictions) in enumerate(model_predictions.items()):\n",
    "    predictions_reversed = pprocessor.reverse_processing(predictions)\n",
    "    num_sequence = predictions_reversed.size(0)\n",
    "    frames = []\n",
    "    for i in range(num_sequence):\n",
    "        # set up a new figure each time.\n",
    "        fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "        ax = fig.add_subplot(projection=ccrs.PlateCarree())\n",
    "        ax.add_feature(cfeature.COASTLINE, edgecolor=\"black\", linewidth=1, alpha=0.75)\n",
    "        ax.add_feature(cfeature.BORDERS, edgecolor=\"darkgrey\", linewidth=1, alpha=0.75)\n",
    "        ax.set_extent(geo_box, crs=ccrs.PlateCarree())\n",
    "        ax.pcolormesh(\n",
    "            lon_grid,\n",
    "            lat_grid,\n",
    "            np.flipud(predictions_reversed[i, c, ...].detach().cpu()),\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            **plot_params,\n",
    "        )\n",
    "        ax.set_title(f\"$x_{{t+{i*60}min}}$\", fontsize=fs)\n",
    "\n",
    "        # Save the figure correctly\n",
    "        fig.canvas.draw()\n",
    "        buf = BytesIO()\n",
    "        fig.savefig(buf, format=\"png\", dpi=100)\n",
    "        buf.seek(0)\n",
    "        image = Image.open(buf)\n",
    "        image = image.convert(\"RGBA\")\n",
    "        frames.append(np.array(image))\n",
    "        plt.close(fig)\n",
    "        buf.close()\n",
    "\n",
    "    print(len(frames))\n",
    "    gif_path = f\"{model_name}_prediction.gif\"\n",
    "    imageio.mimsave(gif_path, frames, fps=2)\n",
    "\n",
    "    gifs[model_name] = frames\n",
    "\n",
    "# also get GIF for targets.\n",
    "frames = []\n",
    "for i in range(num_sequence):\n",
    "    # set up a new figure each time.\n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = fig.add_subplot(projection=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.COASTLINE, edgecolor=\"black\", linewidth=1, alpha=0.75)\n",
    "    ax.add_feature(cfeature.BORDERS, edgecolor=\"darkgrey\", linewidth=1, alpha=0.75)\n",
    "    ax.set_extent(geo_box, crs=ccrs.PlateCarree())\n",
    "    ax.pcolormesh(\n",
    "        lon_grid,\n",
    "        lat_grid,\n",
    "        np.flipud(targets[i, c, ...].detach().cpu()),\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        **plot_params,\n",
    "    )\n",
    "    ax.set_title(f\"$x_{{t+{i*60}min}}$\", fontsize=fs)\n",
    "\n",
    "    # Save the figure correctly\n",
    "    fig.canvas.draw()\n",
    "    buf = BytesIO()\n",
    "    fig.savefig(buf, format=\"png\", dpi=100)\n",
    "    buf.seek(0)\n",
    "    image = Image.open(buf)\n",
    "    image = image.convert(\"RGBA\")\n",
    "    frames.append(np.array(image))\n",
    "    plt.close(fig)\n",
    "    buf.close()\n",
    "\n",
    "gif_path = f\"targets_prediction.gif\"\n",
    "imageio.mimsave(gif_path, frames, fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END OF SCRIPT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irp_rain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
