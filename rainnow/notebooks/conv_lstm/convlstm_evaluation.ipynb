{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ConvLSTM` Evaluation.\n",
    "* The aim of this notebook is to evaluate trained `ConvLSTM` models.\n",
    "\n",
    "* 2 evaluations take place in this NB, the first is an evaluation of the metrics on the entire sequence and the second is metrics `CSI` and `LPIPS` / time.\n",
    "\n",
    "* The `ConvLSTM` models in this NB are roll-out or next-step models. Please see `convlstm_inference_roll_out.ipynb` for more details. A single-shot ConvLSTM was experimented with in `convlstm_inference_one_shot.ipynb` but to make the sampling method more aligned to DYffusion, the roll-out was chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xarray as xr\n",
    "import xskillscore as xs\n",
    "from livelossplot import PlotLosses\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy import io\n",
    "from torch.nn import L1Loss, MSELoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.image import LearnedPerceptualImagePatchSimilarity as LPIPS\n",
    "from torchmetrics.regression import CriticalSuccessIndex\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from rainnow.src.convlstm_trainer import save_checkpoint, train, validate, create_eval_loader\n",
    "from rainnow.src.datasets import IMERGDataset\n",
    "from rainnow.src.plotting import plot_training_val_loss, plot_predicted_sequence\n",
    "from rainnow.src.loss import CBLoss, LPIPSMSELoss\n",
    "from rainnow.src.models.conv_lstm import ConvLSTMModel\n",
    "from rainnow.src.normalise import PreProcess\n",
    "from rainnow.src.utilities.loading import load_imerg_datamodule_from_config\n",
    "from rainnow.src.utilities.utils import (\n",
    "    get_device,\n",
    "    transform_0_1_to_minus1_1,\n",
    "    transform_minus1_1_to_0_1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `helpers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** DIR helpers **\n",
    "BASE_PATH = \"/teamspace/studios/this_studio\"\n",
    "\n",
    "CKPT_BASE_PATH = f\"{BASE_PATH}/DYffcast/rainnow/results/\"\n",
    "CONFIGS_BASE_PATH = f\"{BASE_PATH}/DYffcast/rainnow/src/dyffusion/configs/\"\n",
    "\n",
    "CKPT_DIR = \"checkpoints\"\n",
    "CKPT_CFG_NAME = \"hparams.yaml\"\n",
    "DATAMODULE_CONFIG_NAME = \"imerg_precipitation.yaml\"\n",
    "# whether or not to get last.ckpt or to get the \"best model\" ckpt (the other one in the folder).\n",
    "GET_LAST = False\n",
    "\n",
    "# ** Dataloader Params **\n",
    "BATCH_SIZE = 12\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "INPUT_SEQUENCE_LENGTH = 4\n",
    "OUTPUT_SEQUENCE_LENGTH = 1\n",
    "\n",
    "# ** plotting helpers **\n",
    "# cmap = io.loadmat(\"../../src/utilities/cmaps/colormap.mat\")\n",
    "cmap = io.loadmat(f\"{BASE_PATH}/DYffcast/rainnow/src/utilities/cmaps/colormap.mat\")\n",
    "rain_cmap = ListedColormap(cmap[\"Cmap_rain\"])\n",
    "global_params = {\"font.size\": 8}  # , \"font.family\": \"Times New Roman\"}\n",
    "plt_params = {\"wspace\": 0.1, \"hspace\": 0.15}\n",
    "ylabel_params = {\"ha\": \"right\", \"va\": \"bottom\", \"labelpad\": 1, \"fontsize\": 7.5}\n",
    "\n",
    "# ** get device **\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Instantiate + Load in the datamodule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = load_imerg_datamodule_from_config(\n",
    "    cfg_base_path=CONFIGS_BASE_PATH,\n",
    "    cfg_name=DATAMODULE_CONFIG_NAME,\n",
    "    overrides={\n",
    "        \"boxes\": [\"0,0\", \"1,0\", \"2,0\", \"2,1\"],\n",
    "        \"window\": 1,\n",
    "        \"horizon\": 8,\n",
    "        \"prediction_horizon\": 8,\n",
    "        \"sequence_dt\": 1,\n",
    "    },\n",
    ")\n",
    "\n",
    "datamodule.setup(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Create the test_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the datasets.\n",
    "test_dataset = IMERGDataset(\n",
    "    datamodule, \"test\", sequence_length=INPUT_SEQUENCE_LENGTH, target_length=OUTPUT_SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Instantiate the preprocessor object`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** instantiate the preprocesser obj **\n",
    "pprocessor = PreProcess(\n",
    "    percentiles=datamodule.normalization_hparams[\"percentiles\"],\n",
    "    minmax=datamodule.normalization_hparams[\"min_max\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Get Metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate metrics.\n",
    "lpips = LPIPS(reduction=\"mean\", normalize=True).to(\n",
    "    device\n",
    ")  # set to True so that the function normalises to [-1, 1].\n",
    "mse = MSELoss(reduction=\"mean\")\n",
    "csi_nodes = [2, 10, 18]\n",
    "# need to get the nodes to the same scale as the data. See NB:  imerg_rainfall_classes.ipynb for rain classes + distributions.\n",
    "normed_csi_nodes = pprocessor.apply_preprocessing(np.array(csi_nodes))\n",
    "csi2 = CriticalSuccessIndex(threshold=normed_csi_nodes[0]).to(device)\n",
    "csi10 = CriticalSuccessIndex(threshold=normed_csi_nodes[1]).to(device)\n",
    "csi18 = CriticalSuccessIndex(threshold=normed_csi_nodes[-1]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Evaluation Metrics (entire predictions)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** create the eval dataloader **\n",
    "eval_loader, _ = create_eval_loader(\n",
    "    data_loader=test_loader, horizon=8, input_sequence_length=4, img_dims=(128, 128)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvLSTM params (make sure that they match up with the model checkpoint).\n",
    "KERNEL_SIZE = (5, 5)\n",
    "INPUT_DIMS = (1, 128, 128)  # C, H, W\n",
    "OUTPUT_CHANNELS = 1\n",
    "HIDDEN_CHANNELS = [128, 128]\n",
    "NUM_LAYERS = 2\n",
    "CELL_DROPOUT = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "# conv lstm models to evaluate.\n",
    "conv_lstm_models = {\n",
    "    # ** LCB Loss **\n",
    "    \"convlstm-abcd1234\": (\"I4:T1 | hs=(128, 128), ks=(5, 5), dp=.15 [20E, lr=3e-4] BS=12 LPIPS.\", nn.Tanh()),\n",
    "    \"conv_lstm_f1dlb0m7\": (\"I4:T1 | hs=(128, 128), ks=(5, 5), dp=.4 [30E, lr=7e-5] BS=12 reversal=15% LPIPS.\", nn.Tanh()),\n",
    "    \"conv_lstm_9h86gnt5\": (\"I4:T1 | hs=(128, 128), ks=(5, 5), dp=.40 [20E, lr=7e-5] BS=12 reveral=20% LPIPS.\", nn.Tanh()),\n",
    "    \"conv_lstm_wwj6eryz\": (\"I4:T1 | hs=(128, 128), ks=(5, 5), dp=.40 [10E, lr=7e-5] BS=12 reveral=20% LPIPS.\", nn.Tanh()),\n",
    "    \"conv_lstm_i93g6pzb\": (\"I4:T1 | hs=(128, 128), ks=(5, 5), dp=.45 [10E, lr=3e-4] BS=24 reveral=20% LPIPS.\", nn.Tanh()),\n",
    "    # ** BCE Loss **\n",
    "    \"convlstm-a8kwo8jx\": (\"I4:T1 | hs=(128, 128), ks=(5, 5), dp=.15 [20E, lr=3e-4] BS=12 BCE.\", nn.Sigmoid()),\n",
    "    \"conv_lstm_k34y14il\": (\"I4:T1 | hs=(128, 128), ks=(5, 5), dp=.4 [10E, lr=7e-5] BS=12 BCE.\", nn.Sigmoid()),\n",
    "}\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** evaluate 101 **\n",
    "eval_metrics = {}\n",
    "for ckpt_id, model_desc in conv_lstm_models.items():\n",
    "    # create the model ckpt path.\n",
    "    ckpt_id_path = Path(\n",
    "        os.path.join(CKPT_BASE_PATH, \"convlstm_experiments\", ckpt_id, \"checkpoints\", f\"{ckpt_id}.pt\")\n",
    "    )\n",
    "    print(f\"Loading model ckpt from {ckpt_id_path}.\")\n",
    "\n",
    "    OUTPUT_ACTIVATION = model_desc[-1]\n",
    "    print(f\"Instantiating model w/ output activation = {str(model_desc[-1])}.\")\n",
    "\n",
    "    # instantiate a new ConvLSTM model.\n",
    "    model = ConvLSTMModel(\n",
    "        input_sequence_length=INPUT_SEQUENCE_LENGTH,\n",
    "        output_sequence_length=OUTPUT_SEQUENCE_LENGTH,\n",
    "        input_dims=INPUT_DIMS,\n",
    "        hidden_channels=HIDDEN_CHANNELS,\n",
    "        output_channels=OUTPUT_CHANNELS,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        kernel_size=KERNEL_SIZE,\n",
    "        output_activation=OUTPUT_ACTIVATION,\n",
    "        apply_batchnorm=True,\n",
    "        cell_dropout=CELL_DROPOUT,\n",
    "        bias=True,\n",
    "        device=device,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    # load in the checkpoint + set to eval() mode.\n",
    "    model.load_state_dict(\n",
    "        state_dict=torch.load(ckpt_id_path, map_location=torch.device(device))[\"model_state_dict\"]\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    # ** get preds / target pairs **\n",
    "    # loop through the custom eval_loader and get the predictions and targets for each X, target pair.\n",
    "    # at the end of this loop, you have a results list that contains [target, predictions] pairs.\n",
    "    with torch.no_grad():\n",
    "        results = []\n",
    "        for e, (X, target) in tqdm(\n",
    "            enumerate(eval_loader), total=len(eval_loader), desc=f\"Evaluating model {ckpt_id}\"\n",
    "        ):  # enumerate(eval_loader):\n",
    "            predictions = {}\n",
    "            _input = X.clone().unsqueeze(0).to(device)\n",
    "            for t in range(target.size(0)):\n",
    "                pred = model(_input)  # predict t+1\n",
    "                if isinstance(model.output_activation, nn.Tanh):\n",
    "                    pred = transform_minus1_1_to_0_1(pred)\n",
    "\n",
    "                # add t+i to the predictions.\n",
    "                predictions[f\"t{t+1}\"] = pred.squeeze(0)\n",
    "                # update the inputs with the last pred (auto-regressive rollout)\n",
    "                _input = torch.concat([_input[:, 1:, ...], pred], dim=1)\n",
    "\n",
    "            results.append([target, predictions])\n",
    "\n",
    "        # ** calculate metrics for each preds / target pair **\n",
    "        # reset metrics for eack ckpt id.\n",
    "        # overall metrics for target and prediction.\n",
    "        l2_score = 0\n",
    "        lpips_score = 0\n",
    "        csi2_score = 0  # low rain.\n",
    "        csi10_score = 0  # mid rain.\n",
    "        csi18_score = 0  # heavy rain.\n",
    "        for targets, predictions in results:\n",
    "            # concat to get entire sequence.\n",
    "            pred_seq = torch.cat([v for _, v in predictions.items()], dim=0)\n",
    "\n",
    "            # get metrics.\n",
    "            l2_score += mse(pred_seq.to(device), targets.to(device))\n",
    "            # lpips score. Inputs need to have 3 channels.\n",
    "            lpips_score += lpips(\n",
    "                torch.clamp(pred_seq.expand(-1, 3, -1, -1), 0, 1).to(device),\n",
    "                torch.clamp(targets.expand(-1, 3, -1, -1), 0, 1).to(device),\n",
    "            )\n",
    "            # csi score at different thresholds.\n",
    "            csi2_score += csi2(pred_seq.to(device), targets.to(device))\n",
    "            csi10_score += csi10(pred_seq.to(device), targets.to(device))\n",
    "            csi18_score += csi18(pred_seq.to(device), targets.to(device))\n",
    "\n",
    "        eval_metrics[ckpt_id] = {\n",
    "            \"MSE\": l2_score.item() / len(eval_loader),\n",
    "            \"lpips\": lpips_score.item() / len(eval_loader),\n",
    "            \"csi2\": csi2_score.item() / len(eval_loader),\n",
    "            \"csi10\": csi10_score.item() / len(eval_loader),\n",
    "            \"csi18\": csi18_score.item() / len(eval_loader),\n",
    "        }\n",
    "\n",
    "# create df, format it and export it to a .csv.\n",
    "df_results = pd.DataFrame(eval_metrics).T\n",
    "df_results[[\"MSE\", \"lpips\", \"csi2\", \"csi10\", \"csi18\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Eval Metrics (CSI + LPIPS / t)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics_per_t = {}\n",
    "for ckpt_id, model_desc in conv_lstm_models.items():\n",
    "    # create the model ckpt path.\n",
    "    # ckpt_id_path = Path(os.path.join(CKPT_BASE_PATH, \"\", ckpt_id, \"checkpoints\", f\"{ckpt_id}.pt\"))\n",
    "    ckpt_id_path = Path(os.path.join(BASE_PATH, f\"{ckpt_id}.pt\"))\n",
    "    print(f\"Loading model ckpt from {ckpt_id_path}.\")\n",
    "\n",
    "    OUTPUT_ACTIVATION = model_desc[-1]\n",
    "    print(f\"Instantiating model w/ output activation = {str(model_desc[-1])}.\")\n",
    "\n",
    "    # instantiate a new ConvLSTM model.\n",
    "    model = ConvLSTMModel(\n",
    "        input_sequence_length=INPUT_SEQUENCE_LENGTH,\n",
    "        output_sequence_length=OUTPUT_SEQUENCE_LENGTH,\n",
    "        input_dims=INPUT_DIMS,\n",
    "        hidden_channels=HIDDEN_CHANNELS,\n",
    "        output_channels=OUTPUT_CHANNELS,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        kernel_size=KERNEL_SIZE,\n",
    "        output_activation=OUTPUT_ACTIVATION,\n",
    "        apply_batchnorm=True,\n",
    "        cell_dropout=CELL_DROPOUT,\n",
    "        bias=True,\n",
    "        device=device,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    # load in the checkpoint + set to eval() mode.\n",
    "    model.load_state_dict(\n",
    "        state_dict=torch.load(ckpt_id_path, map_location=torch.device(device))[\"model_state_dict\"]\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    # ** get preds / target pairs **\n",
    "    # loop through the custom eval_loader and get the predictions and targets for each X, target pair.\n",
    "    # at the end of this loop, you have a results list that contains [target, predictions] pairs.\n",
    "    with torch.no_grad():\n",
    "        results = []\n",
    "        for e, (X, target) in tqdm(\n",
    "            enumerate(eval_loader), total=len(eval_loader), desc=f\"Evaluating model {ckpt_id}\"\n",
    "        ):  # enumerate(eval_loader):\n",
    "            predictions = {}\n",
    "            _input = X.clone().unsqueeze(0).to(device)\n",
    "            for t in range(target.size(0)):\n",
    "                pred = model(_input)  # predict t+1\n",
    "                if isinstance(model.output_activation, nn.Tanh):\n",
    "                    pred = transform_minus1_1_to_0_1(pred)\n",
    "\n",
    "                # add t+i to the predictions.\n",
    "                predictions[f\"t{t+1}\"] = pred.squeeze(0)\n",
    "                # update the inputs with the last pred (auto-regressive rollout)\n",
    "                _input = torch.concat([_input[:, 1:, ...], pred], dim=1)\n",
    "\n",
    "            results.append([target, predictions])\n",
    "\n",
    "        # create csi stores.\n",
    "        csi2_score_t = torch.zeros(target.size(0)).to(device)\n",
    "        csi10_score_t = torch.zeros(target.size(0)).to(device)\n",
    "        csi18_score_t = torch.zeros(target.size(0)).to(device)\n",
    "\n",
    "        # perceptual loss scores.\n",
    "        lpips_score_t = torch.zeros(target.size(0)).to(device)\n",
    "\n",
    "        for targets, predictions in results:\n",
    "            for e, (k, v) in enumerate(predictions.items()):\n",
    "                # loop through all the ts and compute the relevant CSI scores.\n",
    "                csi2_score_t[e] += csi2(targets[e].to(device), v[0, ...].to(device))\n",
    "                csi10_score_t[e] += csi10(targets[e].to(device), v[0, ...].to(device))\n",
    "                csi18_score_t[e] += csi18(targets[e].to(device), v[0, ...].to(device))\n",
    "\n",
    "                lpips_score_t[e] += lpips(\n",
    "                    torch.clamp(targets[e].expand(1, 3, -1, -1), 0, 1).to(device),\n",
    "                    torch.clamp(v[0, ...].expand(1, 3, -1, -1), 0, 1).to(device),\n",
    "                )\n",
    "\n",
    "        # normalise the scores.\n",
    "        eval_metrics_per_t[ckpt_id] = {\n",
    "            \"csi2_t\": csi2_score_t / len(eval_loader),\n",
    "            \"csi10_t\": csi10_score_t / len(eval_loader),\n",
    "            \"csi18_t\": csi18_score_t / len(eval_loader),\n",
    "            \"lpips_t\": lpips_score_t / len(eval_loader),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 8\n",
    "\n",
    "# get csi + lpips dfs.\n",
    "df_all = {}\n",
    "for k, v in eval_metrics_per_t.items():\n",
    "    dfs = {}\n",
    "    for metric, values in v.items():\n",
    "        dfs[metric] = pd.DataFrame(\n",
    "            data=[[i.item() for i in values]], columns=[f\"t{i+1}\" for i in range(len(values))]\n",
    "        )\n",
    "    df_all[k] = dfs\n",
    "\n",
    "df_csi2 = pd.DataFrame(columns=[f\"t{i+1}\" for i in range(horizon)])\n",
    "df_csi10 = pd.DataFrame(columns=[f\"t{i+1}\" for i in range(horizon)])\n",
    "df_csi18 = pd.DataFrame(columns=[f\"t{i+1}\" for i in range(horizon)])\n",
    "df_lpips = pd.DataFrame(columns=[f\"t{i+1}\" for i in range(horizon)])\n",
    "\n",
    "for model_name, metrics in df_all.items():\n",
    "    model_name_clean = model_name.rsplit(\".\", 1)[0]\n",
    "\n",
    "    csi2_values = metrics[\"csi2_t\"].iloc[0].values\n",
    "    csi10_values = metrics[\"csi10_t\"].iloc[0].values\n",
    "    csi18_values = metrics[\"csi18_t\"].iloc[0].values\n",
    "    lpips_values = metrics[\"lpips_t\"].iloc[0].values\n",
    "\n",
    "    df_csi2.loc[model_name_clean] = csi2_values\n",
    "    df_csi10.loc[model_name_clean] = csi10_values\n",
    "    df_csi18.loc[model_name_clean] = csi18_values\n",
    "    df_lpips.loc[model_name_clean] = lpips_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csi10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csi18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lpips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END OF SCRIPT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irp_rain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
